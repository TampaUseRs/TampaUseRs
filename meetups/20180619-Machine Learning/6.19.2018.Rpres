Basic Machine Learning Techniques in R
========================================================
author: Minh Pham (minhpham@usf.edu)
date: 6/19/2018
autosize: true

INTRODUCTION
========================================================
- Regular programming example:

```{r, eval = FALSE}
if (User@Use_R) User@Intelligence = "Smart" else User@Intelligence = "Unknown"
```
- Machine Learning example: 
 - log(p/(1-p)) = beta0 + beta1*UseR
```{r, eval = FALSE}
# Given data, estimate beta0, beta1
# probability of being Smart:
p = 1/
  (1 + exp(beta0 + beta1 * User@Use_R))

```

Contents
========================================================
Common R packages and functions for:
- Supervise Learning
 - Classification
 - Regression
 - Time Series Data
 - Feature Selection & dimension reduction
- Unsupervise Learning
 - Clustering
 - Association Studies
- Reinforcement Learning



Supervise Learning - Classification 1
======================================================

**Example**: Letter recognition from images, [link to description](https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.names)
- Black/white images -> 16 numerical attributes
- Goal: predict letter from images
- get data:
<font size = "6">
```{r}
Letter_recognition = read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data",
                              header = FALSE,
                              col.names = c("Letter", paste0("V", 1:16)))
head(Letter_recognition)
```
</font>

Supervise Learning - Classification 2
======================================
- Measure of performance: accuracy
- Importance of Cross-Validation:
  - Fair estimate of accuracy for future predictions
  - prevent over-fitting
  - package `` caret ``

Supervise Learning - Classification 3
======================================
- Pick algorithms:
Either use experience or just put everything into a neural network

<font size = "4">
```{r}
# library(devtools)
# install_github("minh2182000/Mtoolbox")
library(MToolBox)
visual = Plot.VisualizeSupervise(Letter ~ . , data = Letter_recognition)
ggplotly(visual$Plot) -> Plot1
```
</font>
```{r, echo = FALSE}
htmlwidgets::saveWidget(as.widget(Plot1), file = "6.19.2018-figure/Plot1.html")
```
Supervise Learning - Classification 3 (cont)
======================================
<iframe src="Plot1.html" style="position:absolute;height:100%;width:100%"></iframe>

Supervise Learning - Classification 4
======================================

``caret::train()`` function:
  - ``method``: the name of machine learning method
  - ``trControl``: control cross-validation (and other things)
<font size = "5">
```{r}
library(caret, quietly = TRUE)
tail(names(getModelInfo()), 40) # list of ML methods wrapped by caret
length(names(getModelInfo())) 

```
</font>

Supervise Learning - Classification 5
======================================
 ``caret`` in action
<font size = "4">
```{r}
set.seed(9)
RandomForest = train(form = Letter ~ .,data = Letter_recognition,
                     method = "rf",
                     trControl = trainControl(method = "cv", number = 10))
set.seed(9)
# SVM = train(form = Letter ~ .,data = Letter_recognition,
                     # method = "svmPoly",
                     # trControl = trainControl(method = "cv", number = 10))
# accuracy 0.968
set.seed(9)
LDA = train(form = Letter ~ .,data = Letter_recognition,
                     method = "lda",
                     trControl = trainControl(method = "cv", number = 10))
set.seed(9)
KNN = train(form = Letter ~ .,data = Letter_recognition,
                     method = "knn",
                     trControl = trainControl(method = "cv", number = 10))
set.seed(9)
# NeuralNetworks = train(form = Letter ~ .,data = Letter_recognition,
                     # method = "mxnet",
                     # trControl = trainControl(method = "cv", number = 10))
# fail to converge

RandomForest$results[which.max(RandomForest$results$Accuracy),]
LDA$results[which.max(LDA$results$Accuracy),]
KNN$results[which.max(KNN$results$Accuracy),]

```
</font>

Supervise Learning - Classification 6
======================================
Notes on Neural Networks:
  - ``caret`` package:
    - ``method = "nnet"``: 1 hidden layer neural network
    - ``method = "neuralnet"``: multi-layer perceptron with the ability to create network graph
    - ``method = "mxnet"``: convolutional neural network
    - problem: sssslllooowwww
  - Tensorflow: API in R with package ``tensorflow``
  
Supervise Learning - Regression
======================================
Say we want to predict V1
<font size = "5">
```{r}
set.seed(9)
LM = train(form = V1 ~ .,data = Letter_recognition,
                     method = "lm",
                     trControl = trainControl(method = "cv", number = 10))
set.seed(9)
LASSO = train(form = V1 ~ .,data = Letter_recognition,
                     method = "lasso",
                     trControl = trainControl(method = "cv", number = 10))
LM$results[which.max(LM$results$RMSE),]
LASSO$results[which.max(LASSO$results$RMSE),]

# predict new data
predict(LM, newdata = Letter_recognition[1,])
```
</font>

Supervise Learning - Time Series
======================================
<font size = "4">
```{r}
data(economics)
head(economics, 3)
timeSlices = createTimeSlices(economics$unemploy,
                              initialWindow = 100, # number of train
                              horizon = 10, # number of test
                              skip = 75) # to reduce number of slices
library(forecast, quietly = TRUE); library(dplyr, quietly = TRUE)
MSPE = numeric(0)
for (i in 1:length(timeSlices$train) ){ # for each time slice
  ARIMA = auto.arima(ts(economics$unemploy[timeSlices$train[[i]]]), # train
                     xreg = as.matrix(
                       economics 
                        %>% select(pce, pop, psavert)
                        %>% slice(timeSlices$train[[i]])
                     )
                     )
  pred = predict(ARIMA, n.ahead = 10, # test
                 newxreg = as.matrix(
                   economics 
                   %>% select(pce, pop, psavert)
                   %>% slice(timeSlices$test[[i]])
                 ))
  MSPE[i] = mean((pred$pred - economics$unemploy[timeSlices$test[[i]]])^2)
}
mean(sqrt(MSPE))
```
</font>

Supervise Learning - Feature selection
=========================================================
select the original variables
<font size = "5">
```{r}
LDA_RFE = rfe(x = model.matrix(Letter ~ . - 1, data = Letter_recognition),
              y = Letter_recognition$Letter,
              sizes = c(7, 13),
              rfeControl = rfeControl(functions = caretFuncs, # select method in caret
                                      number = 3, # number of resampling
                                      ),
              method = "lda")
LDA_RFE$optVariables
```
</font>


Supervise Learning - dimension reduction
=========================================================
transform original variables then select

<font size = "4">
```{r}
PCA = prcomp(model.matrix(Letter ~ . - 1, data = Letter_recognition))
cat("variance explained according to number of PCs selected \n"); cumsum(PCA$sdev)/sum(PCA$sdev)
NewPredictors = PCA$x[,1:15]
cat("selected PCs \n"); head(NewPredictors, 5)
```
</font>

UnSupervise Learning - Clustering 1
=========================================================
<font size = "5">
When we have no target variable
```{r}
Image_data = Letter_recognition %>% select(-Letter)
```

- Hierachical clustering: good for non-spherical clusters, sensitive to noise, slow, can help decide the number of clusters
```{r}
HC = hclust(dist(Image_data))
plot(HC)
```
</font>

UnSupervise Learning - Clustering 2
=========================================================
<font size = "4">
- K-Means: spherical clusters, fast, results are random, have to specify number of cluster
```{r}
KMEAN = kmeans(Image_data, centers = 26)
KMEAN_result = cbind(cluster = as.factor(KMEAN$cluster), Image_data)
library(MToolBox)
Plot2 <- Plot.VisualizeSupervise(cluster ~ ., data = KMEAN_result)$Plot
```
```{r, echo = FALSE}
htmlwidgets::saveWidget(as.widget(Plot2), file = "6.19.2018-figure/Plot2.html")
```
<iframe src="Plot2.html" style="position:absolute;height:100%;width:100%"></iframe>

</font>

UnSupervise Learning - Clustering 3
=========================================================
How many clusters? We can try to guess by using hierachical
<font size = "5">
```{r}
library(MToolBox)
Plot.ClusterElbow(Image_data, 30)
```
</font>

UnSupervise Learning - Clustering 4
=========================================================
<font size = "5">
Cut the tree for Hierachical
```{r}
clusters = cutree(HC, k = 20) # this is a vector of cluster numbers
```
</font>

<font size = "5">
K-Means:
```{r}
KMEAN = kmeans(Image_data, centers = 20)
clusters = KMEAN$cluster # this is a vector of cluster numbers
```
</font>

Association Studies
=========================================================
- Market Basket
- Genome-wide Association Study
- Drug-Adverse event Association Study

Association Studies 1
=========================================================

Example: Association Rules algorithm
<font size = "4">
```{r}
library(arules)
groceries = read.csv("http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml13/groceries.csv", header = FALSE)
head(groceries,5)
transactions = read.transactions("http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml13/groceries.csv", sep = ",")
summary(transactions)

```
</font>

Association Studies 2
=========================================================
<font size = "6">
```{r}
rules = apriori(transactions, parameter = list(support = 0.001 # parameter for sensitivity
                                       ),
                control = list(verbose = FALSE))

rules_df = as(rules, "data.frame")
head(rules_df$rules[
  order(rules_df$lift, decreasing = TRUE)]
  , 10) # print associated items
```
</font>

Wrap Up
=========================================================
- Supervised Learning: ``caret`` has all you need
  - Will benefit a lot from hyperthreading
  - probably more state-of-the-art algorithms from ``tensorflow`` and ``h2o``
- Unsupervised learning:
  - clustering is in ``base``
  - Association Study hugely depends on the problem
- Still missing: reinforcement learning
  